# RL 拓扑优化实验报告

## 1. 实验背景与目标

### 1.1 项目背景

**TopoDesign（本项目）** 是一个基于强化学习的网络拓扑优化系统，核心组件位于 `rl_topo_opt/` 目录。

设计目标：使用 RL (PPO + GNN) 方法替代传统进化算法（NSGA-II），实现网络拓扑的自动化优化。

### 1.2 技术路线

本项目使用 RL (PPO + GNN) 生成优化的网络拓扑：

```
ATOP generator (外部依赖) → NetTopology → RL 优化 → 优化后拓扑
              ↑                                  
         仅使ATOP用生成器                          
```

**引用关系**：
- `ATOP.generator.network`: `construct_topology` - 拓扑生成（外部依赖）
- `ATOP.NSGAII.solution`: `NetTopology` - 数据结构（外部依赖）

**本项目代码**（`rl_topo_opt/`）：
- `main.py` - 训练入口
- `env/` - RL 环境、奖励函数
- `models/` - GNN、PPO 智能体
- `training/` - 训练循环、检查点管理
- `converter.py` - 拓扑格式转换

### 1.3 实验目的

1. 验证 PPO 算法在拓扑优化任务上的可行性
2. 评估 GNN 状态编码对拓扑结构的表达能力
3. 识别当前系统的问题与瓶颈
4. 为后续优化提供实验依据

### 1.4 实验定位

**本实验定位**：框架搭建验证
- 目标是验证系统能够正常运行
- 不对优化性能抱有期望
- 通过实验发现问题和改进方向

## 2. 实验设置

### 2.1 硬件环境

- CPU：Apple M2 Pro
- 内存：16GB
- 训练环境：Python 3.9 + PyTorch + Stable-Baselines3

### 2.2 软件环境

```
主要依赖：
- gym >= 0.21.0
- numpy
- torch
- stable-baselines3
- tensorboard
```

### 2.3 拓扑配置参数

| 参数 | 值 | 说明 | 来源 |
|------|-----|------|------|
| n_gpus | 64 | GPU 节点数量 | 实验命令参数 |
| depth | 3 | 拓扑深度（总层数） | 实验命令参数 |
| width | 3 | 拓扑宽度（最大维度） | 实验命令参数 |

数据来源：`result/topology_report.txt`

### 2.4 训练超参数

| 参数 | 值 | 说明 | 来源 |
|------|-----|------|------|
| total_timesteps | 100,000 | 训练总步数 | 实验命令参数 |
| learning_rate | 3e-4 | 学习率 | `main.py` 默认值 |
| n_steps | 2048 | 每次更新的采集步数 | `main.py` 默认值 |
| batch_size | 64 | 批次大小 | `main.py` 默认值 |
| n_epochs | 10 | 每次更新的 epoch 数 | `main.py` 默认值 |
| checkpoint_interval | 100 | 检查点保存间隔 | `main.py` 默认值 |

### 2.5 奖励函数配置

三个优化目标权重相等（各 1/3）：
- **Cost**：降低硬件成本
- **Latency**：减少通信延迟
- **Fault Tolerance**：提升容错能力

断连惩罚：-1000

## 3. 实验流程

### 3.1 整体流程

```
┌─────────────────────────────────────────────────────────────────┐
│                      rl_topo_opt 训练流程                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   1. 拓扑生成 (ATOP)                                             │
│      ├── construct_topology()                                    │
│      ├── 生成 GPU-Switch 层级拓扑                                 │
│      └── 输出：NetTopology                                       │
│                              ↓                                    │
│   2. 格式转换 (converter.py)                                     │
│      ├── NetTopology → SimplifiedTopology                       │
│      ├── 提取节点、边、节点类型                              │
│      └── 输出：RL 环境可用的图结构                                 │
│                              ↓                                    │
│   3. RL 训练循环                                                 │
│      ├── TopoEnv.reset() → 状态观测                               │
│      ├── GNN 编码拓扑状态                                         │
│      ├── PPO 选择删除边的动作                                      │
│      ├── 环境执行 → 奖励计算                                       │
│      └── PPO 网络更新                                             │
│                              ↓                                    │
│   4. 检查点保存                                                   │
│      ├── 每 100 步保存                                            │
│      ├── 保留最近 5 个                                           │
│      └── 支持断点续训                                             │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### 3.2 RL 环境设计

**状态空间**：
- 节点特征：节点类型（GPU/SWITCH）+ 度数
- 邻接矩阵：边的连接关系
- 动作掩码：标记无效动作（删除会导致断连的边）

**动作空间**：
- 离散动作：从当前拓扑的所有边中选择一条删除
- 动作掩码：使用 Tarjan 算法识别桥边（Bridge）

**终止条件**：
- 拓扑断连
- 边数过少
- 达到最大步数

### 3.3 GNN 模型结构

```
输入：邻接矩阵 (N×N) + 节点特征 (N×3)
  ↓
GCN Layer 1: 3 → 64
  ↓
GCN Layer 2: 64 → 128
  ↓
GCN Layer 3: 128 → 128
  ↓
Global Mean Pooling
  ↓
输出：图级特征 (128,)
```

### 3.4 训练执行步骤

```bash
# 1. 生成初始拓扑
python main.py --n_gpus 64 --depth 3 --width 3 --total_timesteps 100000

# 2. TensorBoard 监控
tensorboard --logdir ./logs

# 3. 评估训练结果
python main.py --eval_only --resume ./checkpoints/checkpoint_step_100000_*.pkl
```

## 4. 实验结果

### 4.1 实验时间

| 时间点 | 精确时间 | 依据 |
|--------|----------|------|
| 训练开始 | 2026-02-08 18:32:24 | `logs/PPO_11/` 文件创建时间 |
| 训练完成 | 2026-02-08 19:04:49 | `logs/PPO_11/` 文件修改时间 |
| 训练时长 | **约 32 分钟** | - |
| 检查点生成 | 2026-02-08 19:04:48 | `checkpoint_step_100000_20260208_190448` |

### 4.2 拓扑规模

| 指标 | 初始值 | 最终值 | 变化 |
|------|--------|--------|------|
| 节点数 | 75 | 75 | 0 |
| 初始边数 | 541 | 540 | -1 |
| 边数变化率 | - | 0.18% | - |
| **移除边数** | - | **0** | - |

数据来源：`result/topology_report.txt`

### 4.3 性能指标对比

| 指标 | 初始值 | 最终值 | 改进幅度 |
|------|--------|--------|----------|
| Cost | 7298.0000 | 7298.0000 | 0.0% |
| Latency | 1.6154 | 1.6154 | 0.0% |
| Fault Tolerance | 1.5367 | 1.5367 | 0.0% |
| Total | 7298.0787 | 7298.0787 | 0.0% |

数据来源：`result/topology_report.txt`

### 4.4 训练统计

| 统计项 | 值 | 说明 |
|--------|-----|------|
| 训练步数 | 100,352 | `total_timesteps` |
| 单次 rollout 步数 | 2,048 | `n_steps`（每次更新前收集的步数） |
| 总 rollout 次数 | ~49 次 | 100,352 / 2,048 ≈ 49 |
| 单 Episode 最大步数 | 100 | `max_steps` |
| 模型更新频率 | 每 2,048 步 | PPO 标准更新机制 |
| 运行次数 | 11 次 | 独立训练运行 |
| 最后检查点 | step 100,300 | - |
| 平均 Episode 奖励 | ~-1000（断连惩罚） | - |

### 4.5 结果评估

- **移除边数**：0 条边被移除
- **优化效果**：所有性能指标无变化（0.0%）
- **训练结果**：100K 步训练完全未产生任何有效优化

## 5. 问题分析与诊断

### 5.1 核心问题

**拓扑连接度过高导致动作空间被完全掩码**

实验结果显示，RL 智能体在 100K 步训练中几乎未执行任何有效优化动作。根本原因在于 ATOP 生成的初始拓扑具有极高的连接度，几乎所有边都是**桥边（Bridge）**——删除这些边会导致拓扑断连。

### 5.2 问题根源分析

| 问题 | 分析 |
|------|------|
| ATOP 生成策略 | 使用 block-wise 结构生成最小连通图，连接度极低 |
| 动作掩码 | 几乎所有边都是桥边，被标记为无效动作 |
| 奖励信号 | 智能体频繁触发断连惩罚（-1000），无法学习有效策略 |
| 学习困难 | 有效的正向奖励稀缺，智能体难以区分好坏动作 |

### 5.3 技术细节

**Tarjan 算法识别桥边**：
- 当前实现能够正确识别拓扑中的桥边
- 被标记为桥边的动作会被掩码（mask = 0）
- PPO 智能体无法选择这些动作

**断连惩罚机制**：
- 每当拓扑断连，Episode 终止
- 奖励设为 -1000（强惩罚）
- 智能体收到大量负向反馈

## 6. 改进方向

### 6.1 问题总结

当前方案使用**删除边**作为优化动作，但 ATOP 生成的拓扑连接度极低，几乎所有边都是桥边，导致动作空间被完全掩码。

### 6.2 改进思路

#### 思路一：改为增加边

**核心思想**：保留 ATOP 的节点结构（交换机层级），动作空间从"删除边"变为"增加边"。

| 方面 | 当前方案 | 改进方案 |
|------|----------|----------|
| 动作 | 删除边 | 增加边 |
| 拓扑变化 | 减少连接 | 增加连接 |
| 约束 | 保持连通 | 无需额外约束 |
| 目标 | 精简拓扑 | 增强拓扑性能 |

**动作空间**：连接任意两个节点（GPU-GPU、GPU-Switch、Switch-Switch）

**优点**：
- 动作空间不受桥边限制
- 可以逐步增强拓扑连接度
- 更符合"优化"直觉（从简单到复杂）

#### 思路二：全连接后删边

**核心思想**：先生成全连接图，然后逐步删除边进行优化。

| 方面 | 当前方案 | 改进方案 |
|------|----------|----------|
| 初始状态 | ATOP 稀疏拓扑 | 全连接拓扑 |
| 动作 | 删除边 | 删除边 |
| 动作有效性 | 多数是桥边 | 多数非桥边 |
| 训练难度 | 极高 | 较低 |

**优点**：
- 全连接图有丰富的非桥边可供删除
- 奖励信号更加多样
- 更容易学习有效的删除策略

### 6.3 两种思路对比

| 维度 | 思路一（增边） | 思路二（全连接） |
|------|---------------|-----------------|
| 动作类型 | 增加连接 | 删除连接 |
| 拓扑规模 | 逐渐增大 | 逐渐减小 |
| 计算复杂度 | 边数增加，GCN 变慢 | 边数减少，GCN 变快 |
| 物理意义 | 增强连接 | 精简冗余 |
| 实现难度 | 中等 | 较低 |

### 6.4 共同约束（暂忽略）

- GPU 端口数：单个
- Switch 端口数：32 或 64 个

## 7. 结论与下一步计划

### 7.1 实验结论

1. **框架验证成功**：rl_topo_opt 系统能够正常运行，拓扑生成、转换、训练、评估流程完整
2. **当前方案失效**：删除边策略在 ATOP 生成的稀疏拓扑上完全失效
3. **问题定位清晰**：根本原因在于拓扑连接度低，动作空间被桥边掩码
4. **改进方向明确**：两种改进思路都具有可行性