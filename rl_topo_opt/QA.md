# RL 拓扑优化 - QA 文档

## Q1: reward 计算时机？是不是每次选择完一个 action 都会计算一次？

**Answer**:

是的，每次选择完 action 都会计算一次奖励。

**调用流程**（`env/topo_env.py:86-132`）：
```
PPO 选择动作 (action)
       ↓
env.step(action)
       ↓
移除边 → 检查连通性
       ↓
reward_calculator.calculate_reward()  ← 每次 step 时调用
       ↓
返回 (observation, reward, done, info)
```

**计算时机**：
- 每次 `step()` 被调用时
- 基于**执行动作后的当前拓扑状态**计算
- 如果拓扑断连，返回 -1000 惩罚
- 否则返回三指标加权奖励

## Q2: RL 的 observation、action logits 是如何得到的？完整数据流是什么？

**Answer**:

### 完整数据流图

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              RL 训练/推理数据流                                       │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ┌──────────────────────────────────────────────────────────────────────────┐       │
│  │                           Environment (TopoEnv)                            │       │
│  │  ┌─────────────────────┐  ┌─────────────────────┐  ┌───────────────────┐ │       │
│  │  │  node_features      │  │  adjacency_matrix   │  │  action_mask      │ │       │
│  │  │  (N, 3)            │  │  (N, N)            │  │  (E₀,)            │ │       │
│  │  │  GPU=1/SWITCH=0     │  │  0/1               │  │  1=有效/0=无效   │ │       │
│  │  │  degree, layer      │  │  无向边邻接矩阵     │  │  桥边掩码        │ │       │
│  │  └─────────────────────┘  └─────────────────────┘  └───────────────────┘ │       │
│  └──────────────────────────────────────────────────────────────────────────┘       │
│                                      ↓                                             │
│                              Dict Observation                                      │
│                                      ↓                                             │
│  ┌──────────────────────────────────────────────────────────────────────────┐       │
│  │                     GNNFeaturesExtractor (SB3 Wrapper)                    │       │
│  │                                                                          │       │
│  │   obs['node_features'] ──→ obs_to_pyg_data() ──→ PyG Data               │       │
│  │                               (N, 3)                     ↓              │       │
│  │   obs['adjacency_matrix'] ─→ edge_index (2, 2E)                         │       │
│  │                                                                          │       │
│  │   ┌──────────────────────────────────────────────────────────────────┐   │       │
│  │   │              GNNFeatureExtractor (3层 GCN)                       │   │       │
│  │   │   x: (N, 3) ──→ GCNConv1 (3→64) ──→ BN+ReLU+Dropout          │   │       │
│  │   │   edge_index: (2, 2E)                                            │   │       │
│  │   │                                  x: (N, 64) ──→ GCNConv2 (64→64) │   │       │
│  │   │   ... 继续 3 层 ...                                              │   │       │
│  │   │                                  x: (N, 64) ──→ GCNConv3 (64→128) │   │       │
│  │   │   x: (N, 128) ──→ Global Mean Pooling ──→ (1, 128)              │   │       │
│  │   └──────────────────────────────────────────────────────────────────┘   │       │
│  └──────────────────────────────────────────────────────────────────────────┘       │
│                                      ↓                                             │
│                              Features: (1, 128)                                  │
│                                      ↓                                             │
│  ┌──────────────────────────────────────────────────────────────────────────┐       │
│  │                          MLP Extractor                                    │       │
│  │   features (1, 128) ──→ Shared MLP ──→ latent_pi (128), latent_vf (128) │       │
│  └──────────────────────────────────────────────────────────────────────────┘       │
│                                      ↓                                             │
│  ┌──────────────────────────┐  ┌─────────────────────────────────────────────┐       │
│  │      Actor Head          │  │              Critic Head                   │       │
│  │   linear(128→128)       │  │   linear(128→128)                        │       │
│  │   → ReLU                │  │   → ReLU                                  │       │
│  │   → linear(128→E₀)     │  │   → linear(128→1)                        │       │
│  │   → logits (E₀,)       │  │   → value (1,)                           │       │
│  └──────────────────────────┘  └─────────────────────────────────────────────┘       │
│                                      ↓                                             │
│  ┌──────────────────────────────────────────────────────────────────────────┐       │
│  │                       MaskedActorCriticPolicy                            │       │
│  │   logits + mask_log_probs ──→ Categorical Distribution                  │       │
│  │   action_mask (E₀,) ──→ torch.log(action_mask + 1e-8) ──→ + logits    │       │
│  └──────────────────────────────────────────────────────────────────────────┘       │
│                                      ↓                                             │
│  ┌──────────────────────────┐  ┌─────────────────────────────────────────────┐       │
│  │      action logits       │  │                  value                      │       │
│  │      shape: (E₀,)       │  │               shape: (1,)                    │       │
│  └──────────────────────────┘  └─────────────────────────────────────────────┘       │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### 维度汇总表

| 模块 | 输入 | 输出 | 形状 |
|------|------|------|------|
| **Environment** | - | `node_features` | (N, 3) |
| | - | `adjacency_matrix` | (N, N) |
| | - | `action_mask` | (E₀,) |
| **obs_to_pyg_data** | `node_features` | `x` | (N, 3) |
| | `adjacency_matrix` | `edge_index` | (2, 2E) |
| **GNN Layer 1** | (N, 3) | (N, 64) | GCNConv |
| **GNN Layer 2** | (N, 64) | (N, 64) | GCNConv |
| **GNN Layer 3** | (N, 64) | (N, 128) | GCNConv |
| **Global Pooling** | (N, 128) | (1, 128) | Mean |
| **MLP Extractor** | (1, 128) | `(1, 128), (1, 128)` | latent_pi, latent_vf |
| **Actor Head** | (1, 128) | (1, E₀) | logits |
| **Critic Head** | (1, 128) | (1, 1) | value |

**符号说明**：
- N：节点数（本实验 75）
- E₀：初始边数（本实验 541）
- E：当前边数（随训练递减）

## Q1: reward 计算时机？是不是每次选择完一个 action 都会计算一次？

**Answer**:

是的，每次选择完 action 都会计算一次奖励。

**调用流程**（`env/topo_env.py:86-132`）：
```
PPO 选择动作 (action)
       ↓
env.step(action)
       ↓
移除边 → 检查连通性
       ↓
reward_calculator.calculate_reward()  ← 每次 step 时调用
       ↓
返回 (observation, reward, done, info)
```

**计算时机**：
- 每次 `step()` 被调用时
- 基于**执行动作后的当前拓扑状态**计算
- 如果拓扑断连，返回 -1000 惩罚
- 否则返回三指标加权奖励
